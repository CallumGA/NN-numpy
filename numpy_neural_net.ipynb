{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Classification Neural Net - NUMPY\n",
    "\n",
    "In this notebook I will develop a simple classification neural network from scratch using pythons NUMPY, instead of relying on libaries like pytorch."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T13:13:48.876336Z",
     "start_time": "2025-08-09T13:13:48.873737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "id": "807f76eb6a7a6401",
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First we will define the constants that will be used throughout the notebook.",
   "id": "1580e508617d8033"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T13:13:48.893171Z",
     "start_time": "2025-08-09T13:13:48.889781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bias1 = 0\n",
    "bias2 = 0\n",
    "learning_rate = 0.1\n",
    "truth = 1 # the value we expect (the actual value that's labeled )"
   ],
   "id": "972f031b596341a7",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next, lets create some sample data to work with.\n",
    "- features: is (n_samples, n_features)\n",
    "- labels: is (n_samples, 1)\n"
   ],
   "id": "f8964909ebe70b81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T13:13:48.914492Z",
     "start_time": "2025-08-09T13:13:48.910851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 8\n",
    "n_features = 3\n",
    "features_matrix = np.random.rand(n_samples, n_features)\n",
    "labels = np.array([[0], [1], [0], [1], [0], [1], [0], [1]])\n",
    "weights1 = np.random.randn(n_features, 4)\n",
    "weights2 = np.random.randn(4, 1)\n",
    "print(f\"feature matrix shape: {features_matrix.shape}\")\n",
    "print(features_matrix)\n",
    "print(f\"weights matrix shape: {weights1.shape}\")\n",
    "print(weights1)\n",
    "print(f\"labels matrix shape: {labels.shape}\")\n",
    "print(labels)"
   ],
   "id": "7f046f99ac7d0fc6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature matrix shape: (8, 3)\n",
      "[[0.37454012 0.95071431 0.73199394]\n",
      " [0.59865848 0.15601864 0.15599452]\n",
      " [0.05808361 0.86617615 0.60111501]\n",
      " [0.70807258 0.02058449 0.96990985]\n",
      " [0.83244264 0.21233911 0.18182497]\n",
      " [0.18340451 0.30424224 0.52475643]\n",
      " [0.43194502 0.29122914 0.61185289]\n",
      " [0.13949386 0.29214465 0.36636184]]\n",
      "weights matrix shape: (3, 4)\n",
      "[[ 1.46564877 -0.2257763   0.0675282  -1.42474819]\n",
      " [-0.54438272  0.11092259 -1.15099358  0.37569802]\n",
      " [-0.60063869 -0.29169375 -0.60170661  1.85227818]]\n",
      "labels matrix shape: (8, 1)\n",
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Neural Network ***Forward Pass*** – 1 Hidden Layer\n",
    "\n",
    "We will create functions for each part of the forward pass:\n",
    "1. **Hidden layer linear transformation** – multiply the feature matrix by the weight matrix, add bias, and produce the pre-activation values for the hidden layer.\n",
    "2. **Hidden layer activation (ReLU)** – introduce non-linearity so the network can learn complex patterns.\n",
    "3. **Output layer linear transformation** – take the hidden layer activations, multiply by the output layer weights, add bias, and produce the output logits.\n",
    "4. **Output layer activation (Sigmoid)** – squash the logits into the range (0, 1) to get probabilities.\n",
    "5. **Loss (MSE)** – measure how far the predicted values are from the target labels.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Hidden layer linear transformation\n",
    "$$\n",
    "Z1 = \\text{features\\_matrix} \\cdot \\text{weights\\_matrix} + \\text{bias}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `features_matrix` = input data `(n_samples, n_features)`\n",
    "- `weights_matrix` = hidden layer weights `(n_features, n_hidden)`\n",
    "- `bias` = hidden layer bias `(1, n_hidden)`\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Hidden layer activation (ReLU)\n",
    "$$\n",
    "A1 = \\max(0, Z1)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `A1` = hidden layer activation output `(n_samples, n_hidden)`\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Output layer linear transformation\n",
    "$$\n",
    "Z2 = A1 \\cdot W2 + b2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `W2` = output layer weights `(n_hidden, 1)`\n",
    "- `b2` = output layer bias `(1, 1)`\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Output layer activation (Sigmoid)\n",
    "$$\n",
    "y_{\\text{pred}} = \\frac{1}{1 + e^{-Z2}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `y_pred` = predicted probabilities `(n_samples, 1)`\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Binary Cross-Entropy (BCE) Loss\n",
    "$$\n",
    "\\text{loss} = -\\frac{1}{n_{\\text{samples}}} \\sum_{i=1}^{n_{\\text{samples}}}\n",
    "\\Big[\\, \\text{labels}_i \\,\\log\\!\\big((y_{\\text{pred}})_i\\big)\n",
    "+ \\big(1 - \\text{labels}_i\\big)\\,\\log\\!\\big(1 - (y_{\\text{pred}})_i\\big) \\Big]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `labels` = true labels `(n_samples, 1)`\n",
    "- `y_pred` = predicted probabilities `(n_samples, 1)` from the sigmoid output\n",
    "- `n_samples` = number of rows in `features_matrix`\n",
    "- (Numerical stability) Clip probabilities before logging:\n",
    "$$\n",
    "y_{\\text{pred}} \\leftarrow \\operatorname{clip}\\!\\big(y_{\\text{pred}}, \\epsilon, 1 - \\epsilon\\big), \\quad \\epsilon \\approx 10^{-15}\n",
    "$$\n",
    "\n",
    "\n"
   ],
   "id": "488cce66cffe9684"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will implement each formula above in order of how they're applied during the forward pass:",
   "id": "887a551efcb590a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T13:13:48.934668Z",
     "start_time": "2025-08-09T13:13:48.931693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def hidden_layer_output_transformation(features_matrix, weights1, bias):\n",
    "    return features_matrix @ weights1 + bias\n",
    "\n",
    "def hidden_ReLU_activation(hidden_layer_output):\n",
    "    return np.maximum(0, hidden_layer_output)\n",
    "\n",
    "def output_layer_transformation(hidden_ReLU_activation, weights2, bias2):\n",
    "    return hidden_ReLU_activation @ weights2 + bias2\n",
    "\n",
    "def output_sigmoid_activation(output_layer_logits):\n",
    "    return 1.0 / (1.0 + np.exp(-output_layer_logits))\n",
    "\n",
    "def BCE_loss():\n",
    "    print(\"BCE loss\")\n",
    "\n",
    "print(\"Hidden layer transformation output: \")\n",
    "hidden_layer_output = hidden_layer_output_transformation(features_matrix, weights1, bias1)\n",
    "print(hidden_layer_output)\n",
    "\n",
    "print(\"Hidden layer ReLU activation: \")\n",
    "hidden_ReLU_activation_output = hidden_ReLU_activation(hidden_layer_output)\n",
    "print(hidden_ReLU_activation(hidden_layer_output))\n",
    "\n",
    "print(\"Output layer transformation output logits: \")\n",
    "output_layer_transformation_logits = output_layer_transformation(hidden_ReLU_activation_output, weights2, bias2)\n",
    "print(output_layer_transformation_logits)\n",
    "\n",
    "print(\"Sigmoid activation output: \")\n",
    "output_sigmoid_activation_output = output_sigmoid_activation(output_layer_transformation_logits)\n",
    "print(output_sigmoid_activation_output)"
   ],
   "id": "9be632b40e61654",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer transformation output: \n",
      "[[-0.40827206 -0.19262465 -1.50941963  1.17941254]\n",
      " [ 0.69879287 -0.16335953 -0.23301305 -0.50537645]\n",
      " [-0.74745409 -0.09237689 -1.35473578  1.35609836]\n",
      " [ 0.44401448 -0.44049936 -0.55947892  0.79545129]\n",
      " [ 0.99526368 -0.21742982 -0.29759288 -0.76945534]\n",
      " [-0.21200664 -0.16072923 -0.65354531  0.82499286]\n",
      " [ 0.10703705 -0.24369272 -0.67419033  0.6273231 ]\n",
      " [-0.17464059 -0.10595443 -0.54727919  0.58961859]]\n",
      "Hidden layer ReLU activation: \n",
      "[[0.         0.         0.         1.17941254]\n",
      " [0.69879287 0.         0.         0.        ]\n",
      " [0.         0.         0.         1.35609836]\n",
      " [0.44401448 0.         0.         0.79545129]\n",
      " [0.99526368 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.82499286]\n",
      " [0.10703705 0.         0.         0.6273231 ]\n",
      " [0.         0.         0.         0.58961859]]\n",
      "Output layer transformation output logits: \n",
      "[[-1.4398783 ]\n",
      " [-0.00943176]\n",
      " [-1.65558408]\n",
      " [-0.97711462]\n",
      " [-0.0134333 ]\n",
      " [-1.00718729]\n",
      " [-0.76730812]\n",
      " [-0.71983211]]\n",
      "Sigmoid activation output: \n",
      "[[0.19156419]\n",
      " [0.49764208]\n",
      " [0.16035567]\n",
      " [0.27346468]\n",
      " [0.49664173]\n",
      " [0.26753066]\n",
      " [0.3170617 ]\n",
      " [0.32742995]]\n"
     ]
    }
   ],
   "execution_count": 97
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
