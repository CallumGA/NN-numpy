{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Classification Neural Net - NUMPY\n",
    "\n",
    "In this notebook I will develop a simple classification neural network from scratch using pythons NUMPY, instead of relying on libaries like pytorch."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T21:55:01.335395Z",
     "start_time": "2025-08-09T21:55:01.333063Z"
    }
   },
   "cell_type": "code",
   "source": "import numpy as np",
   "id": "807f76eb6a7a6401",
   "outputs": [],
   "execution_count": 286
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First we will define the hyperparameters that will be used throughout the notebook.",
   "id": "1580e508617d8033"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T21:55:01.352455Z",
     "start_time": "2025-08-09T21:55:01.350980Z"
    }
   },
   "cell_type": "code",
   "source": "learning_rate = 0.05",
   "id": "972f031b596341a7",
   "outputs": [],
   "execution_count": 287
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next, lets create some sample data to work with.\n",
    "- features: is (n_samples, n_features)\n",
    "- labels: is (n_samples, 1)\n"
   ],
   "id": "f8964909ebe70b81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T21:55:01.366099Z",
     "start_time": "2025-08-09T21:55:01.363006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# features: [height (m), weight (kg), score (0-10)]\n",
    "input_feature_matrix = np.array([\n",
    "    [1.80, 80, 8],   # good\n",
    "    [1.65, 70, 6],   # good\n",
    "    [1.75, 95, 5],   # bad\n",
    "    [1.60, 60, 4],   # bad\n",
    "    [1.82, 77, 9],   # good\n",
    "    [1.55, 50, 3],   # bad\n",
    "    [1.78, 85, 7],   # good\n",
    "    [1.62, 65, 2],   # bad\n",
    "], dtype=float)\n",
    "\n",
    "# normalize features (z-score)\n",
    "X = input_feature_matrix\n",
    "X = (X - X.mean(axis=0, keepdims=True)) / (X.std(axis=0, keepdims=True) + 1e-8)\n",
    "input_feature_matrix = X\n",
    "\n",
    "# labels: 1 = Good, 0 = Bad\n",
    "target_labels = np.array([\n",
    "    [1],\n",
    "    [1],\n",
    "    [0],\n",
    "    [0],\n",
    "    [1],\n",
    "    [0],\n",
    "    [1],\n",
    "    [0]\n",
    "], dtype=float)\n",
    "\n",
    "# random initial weights and biases\n",
    "np.random.seed(42)\n",
    "hidden_layer_weights = np.random.randn(input_feature_matrix.shape[1], 4)\n",
    "output_layer_weights = np.random.randn(4, 1)\n",
    "hidden_layer_bias = np.zeros((1, hidden_layer_weights.shape[1]))\n",
    "output_layer_bias = np.zeros((1, output_layer_weights.shape[1]))\n",
    "\n",
    "num_samples = input_feature_matrix.shape[0]\n",
    "\n",
    "print(f\"feature matrix shape: {input_feature_matrix.shape}\")\n",
    "print(input_feature_matrix)\n",
    "print(f\"hidden weights matrix shape: {hidden_layer_weights.shape}\")\n",
    "print(hidden_layer_weights)\n",
    "print(f\"output weights matrix shape: {output_layer_weights.shape}\")\n",
    "print(output_layer_weights)\n",
    "print(f\"labels matrix shape: {target_labels.shape}\")\n",
    "print(target_labels)"
   ],
   "id": "7f046f99ac7d0fc6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature matrix shape: (8, 3)\n",
      "[[ 1.07448419  0.53602696  1.09108945]\n",
      " [-0.47898693 -0.20332057  0.21821789]\n",
      " [ 0.55666048  1.64504827 -0.21821789]\n",
      " [-0.99681063 -0.94266811 -0.65465367]\n",
      " [ 1.28161367  0.3142227   1.52752522]\n",
      " [-1.51463434 -1.68201564 -1.09108945]\n",
      " [ 0.86735471  0.90570073  0.65465367]\n",
      " [-0.78968115 -0.57299434 -1.52752522]]\n",
      "hidden weights matrix shape: (3, 4)\n",
      "[[ 0.49671415 -0.1382643   0.64768854  1.52302986]\n",
      " [-0.23415337 -0.23413696  1.57921282  0.76743473]\n",
      " [-0.46947439  0.54256004 -0.46341769 -0.46572975]]\n",
      "output weights matrix shape: (4, 1)\n",
      "[[ 0.24196227]\n",
      " [-1.91328024]\n",
      " [-1.72491783]\n",
      " [-0.56228753]]\n",
      "labels matrix shape: (8, 1)\n",
      "[[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "execution_count": 288
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Neural Network ***Forward Pass*** – 1 Hidden Layer\n",
    "\n",
    "We will create functions for each part of the forward pass:\n",
    "1. **Hidden layer linear transformation** – multiply the feature matrix by the weight matrix, add bias, and produce the pre-activation values for the hidden layer.\n",
    "2. **Hidden layer activation (ReLU)** – introduce non-linearity so the network can learn complex patterns.\n",
    "3. **Output layer linear transformation** – take the hidden layer activations, multiply by the output layer weights, add bias, and produce the output logits.\n",
    "4. **Output layer activation (Sigmoid)** – squash the logits into the range (0, 1) to get probabilities.\n",
    "5. **Loss (MSE)** – measure how far the predicted values are from the target labels.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Hidden layer linear transformation\n",
    "$$\n",
    "Z1 = \\text{features\\_matrix} \\cdot \\text{weights\\_matrix} + \\text{bias}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `features_matrix` = input data `(n_samples, n_features)`\n",
    "- `weights_matrix` = hidden layer weights `(n_features, n_hidden)`\n",
    "- `bias` = hidden layer bias `(1, n_hidden)`\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Hidden layer activation (ReLU)\n",
    "$$\n",
    "A1 = \\max(0, Z1)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `A1` = hidden layer activation output `(n_samples, n_hidden)`\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Output layer linear transformation\n",
    "$$\n",
    "Z2 = A1 \\cdot W2 + b2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `W2` = output layer weights `(n_hidden, 1)`\n",
    "- `b2` = output layer bias `(1, 1)`\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Binary Cross-Entropy (BCE) Loss – from logits\n",
    "$$\n",
    "\\text{loss} = \\frac{1}{n_{\\text{samples}}} \\sum_{i=1}^{n_{\\text{samples}}}\n",
    "\\left[ \\max(z_i, 0) - z_i \\cdot \\text{labels}_i + \\log\\left( 1 + e^{-\\lvert z_i \\rvert} \\right) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `z` = logits `(n_samples, 1)` from the output layer transformation (before sigmoid)\n",
    "- `labels` = true labels `(n_samples, 1)`\n",
    "- `n_samples` = number of rows in `features_matrix`\n",
    "- This formulation is **numerically stable** and does **not** require applying the sigmoid in the forward pass.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "488cce66cffe9684"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will implement each formula above in order of how they're applied during the forward pass:",
   "id": "887a551efcb590a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T21:55:01.387423Z",
     "start_time": "2025-08-09T21:55:01.384178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def hidden_layer_output_transformation(input_feature_matrix, hidden_layer_weights, hidden_layer_bias):\n",
    "    return input_feature_matrix @ hidden_layer_weights + hidden_layer_bias\n",
    "\n",
    "def hidden_ReLU_activation(hidden_layer_linear_output):\n",
    "    return np.maximum(0, hidden_layer_linear_output)\n",
    "\n",
    "def output_layer_transformation(hidden_layer_activation_output, output_layer_weights, output_layer_bias):\n",
    "    return hidden_layer_activation_output @ output_layer_weights + output_layer_bias\n",
    "\n",
    "def BCE_loss(output_layer_linear_output, target_labels):\n",
    "    return np.mean(\n",
    "        np.maximum(output_layer_linear_output, 0)\n",
    "        - output_layer_linear_output * target_labels\n",
    "        + np.log1p(np.exp(-np.abs(output_layer_linear_output)))\n",
    "    )\n",
    "\n",
    "print(\"Hidden layer transformation output (pre-activation): \")\n",
    "hidden_layer_linear_output = hidden_layer_output_transformation(input_feature_matrix, hidden_layer_weights, hidden_layer_bias)\n",
    "print(hidden_layer_linear_output)\n",
    "\n",
    "print(\"Hidden layer ReLU activation (post-activation): \")\n",
    "hidden_layer_activation_output = hidden_ReLU_activation(hidden_layer_linear_output)\n",
    "print(hidden_layer_activation_output)\n",
    "\n",
    "print(\"Output layer transformation output logits (pre-sigmoid): \")\n",
    "output_layer_linear_output = output_layer_transformation(hidden_layer_activation_output, output_layer_weights, output_layer_bias)\n",
    "print(output_layer_linear_output)\n",
    "\n",
    "print(\"BCE loss first forward pass:\")\n",
    "first_forward_pass_loss = BCE_loss(output_layer_linear_output, target_labels)\n",
    "print(first_forward_pass_loss)"
   ],
   "id": "9be632b40e61654",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer transformation output (pre-activation): \n",
      "[[-0.10403957  0.31791501  1.03680159  1.53968439]\n",
      " [-0.2927591   0.23222796 -0.73244683 -0.98717722]\n",
      " [-0.00624475 -0.58052918  3.05954995  2.21190827]\n",
      " [ 0.0329421   0.00334785 -1.83091829 -1.93671691]\n",
      " [-0.15411463  0.57800159  0.61842879  1.48166735]\n",
      " [ 0.15374788  0.01126034 -3.13164181 -3.08951772]\n",
      " [-0.08858865  0.02320672  1.68869181  1.71118162]\n",
      " [ 0.45905672 -0.58543029 -0.70846522 -0.93102978]]\n",
      "Hidden layer ReLU activation (post-activation): \n",
      "[[0.         0.31791501 1.03680159 1.53968439]\n",
      " [0.         0.23222796 0.         0.        ]\n",
      " [0.         0.         3.05954995 2.21190827]\n",
      " [0.0329421  0.00334785 0.         0.        ]\n",
      " [0.         0.57800159 0.61842879 1.48166735]\n",
      " [0.15374788 0.01126034 0.         0.        ]\n",
      " [0.         0.02320672 1.68869181 1.71118162]\n",
      " [0.45905672 0.         0.         0.        ]]\n",
      "Output layer transformation output logits (pre-sigmoid): \n",
      "[[-3.26240339e+00]\n",
      " [-4.44317169e-01]\n",
      " [-6.52120071e+00]\n",
      " [ 1.56537927e-03]\n",
      " [-3.00574093e+00]\n",
      " [ 1.56569899e-02]\n",
      " [-3.91943165e+00]\n",
      " [ 1.11074407e-01]]\n",
      "BCE loss first forward pass:\n",
      "1.672443319506854\n"
     ]
    }
   ],
   "execution_count": 289
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Neural Network ***Backpropagation*** – 1 Hidden Layer (BCE from logits)\n",
    "\n",
    "We will compute gradients for each parameter using the chain rule, then update the weights and biases.\n",
    "1. **Gradient of loss w.r.t. output logits** – Figure out how much each output logit (before sigmoid) is pushing the loss up or down, so we know the direction to adjust them.\n",
    "2. **Gradient w.r.t. output layer weights & bias** – See how much each connection from the hidden layer to the output contributed to the error, so we can strengthen or weaken them.\n",
    "3. **Gradient w.r.t. hidden layer activations** – Work backwards to see how much the hidden neurons themselves are responsible for the error at the output.\n",
    "4. **Gradient w.r.t. hidden layer weights & bias** – Determine how much each connection from the inputs to the hidden neurons needs to be adjusted to fix the error.\n",
    "5. **Update weights & biases** – Apply the changes (scaled by the learning rate) so the network gets a bit better at predicting next time.\n",
    "---\n",
    "\n",
    "#### 1. Gradient of loss w.r.t. output layer logits\n",
    "$$\n",
    "dZ2 \\;=\\; \\frac{\\sigma(Z2) - \\text{labels}}{n_{\\text{samples}}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `Z2` = output logits `(n_samples, 1)`\n",
    "- `labels` = true labels `(n_samples, 1)`\n",
    "- `σ(·)` = sigmoid applied element-wise\n",
    "- `n_samples` = number of rows in `features_matrix`\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Output layer parameter gradients\n",
    "$$\n",
    "dW2 \\;=\\; A1^\\top \\cdot dZ2\n",
    "\\qquad\\qquad\n",
    "db2 \\;=\\; \\sum_{i=1}^{n_{\\text{samples}}} (dZ2)_i\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `A1` = hidden layer ReLU output `(n_samples, n_hidden)`\n",
    "- `W2` = output layer weights `(n_hidden, 1)`\n",
    "- `b2` = output layer bias `(1, 1)`\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Backprop into hidden activations\n",
    "$$\n",
    "dA1 \\;=\\; dZ2 \\cdot W2^\\top\n",
    "\\qquad\\qquad\n",
    "dZ1 \\;=\\; dA1 \\odot \\mathbf{1}(Z1 > 0)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `W2.T` = transpose of `W2` `(1, n_hidden)`\n",
    "- `⊙` denotes element-wise product\n",
    "- `Z1` = hidden pre-activations `(n_samples, n_hidden)`\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Hidden layer parameter gradients\n",
    "$$\n",
    "dW1 \\;=\\; X^\\top \\cdot dZ1\n",
    "\\qquad\\qquad\n",
    "db1 \\;=\\; \\sum_{i=1}^{n_{\\text{samples}}} (dZ1)_i\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `X` = input features `(n_samples, n_features)`\n",
    "- `W1` = hidden layer weights `(n_features, n_hidden)`\n",
    "- `b1` = hidden layer bias `(1, n_hidden)`\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Parameter updates (gradient descent)\n",
    "$$\n",
    "W1 \\leftarrow W1 - \\eta \\cdot dW1\n",
    "\\qquad\n",
    "b1 \\leftarrow b1 - \\eta \\cdot db1\n",
    "$$\n",
    "$$\n",
    "W2 \\leftarrow W2 - \\eta \\cdot dW2\n",
    "\\qquad\n",
    "b2 \\leftarrow b2 - \\eta \\cdot db2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `η` = learning rate `(scalar)`\n"
   ],
   "id": "be85d5fe489df2dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T21:55:01.420535Z",
     "start_time": "2025-08-09T21:55:01.417133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def stable_sigmoid(x):\n",
    "    pos = x >= 0\n",
    "    out = np.empty_like(x, dtype=float)\n",
    "    out[pos] = 1.0 / (1.0 + np.exp(-x[pos]))\n",
    "    e = np.exp(x[~pos])\n",
    "    out[~pos] = e / (1.0 + e)\n",
    "    return out\n",
    "\n",
    "# 1) How much each logit is pushing the loss up or down\n",
    "loss_gradient_wrt_logits = (\n",
    "    stable_sigmoid(output_layer_linear_output) - target_labels\n",
    ") / num_samples\n",
    "\n",
    "# 2) How much each output layer weight and bias contributed to the error\n",
    "output_layer_weight_gradients = hidden_layer_activation_output.T @ loss_gradient_wrt_logits\n",
    "output_layer_bias_gradients = np.sum(loss_gradient_wrt_logits, axis=0, keepdims=True)\n",
    "\n",
    "# 3) How much each hidden activation was responsible for the output error\n",
    "hidden_layer_activation_gradients = loss_gradient_wrt_logits @ output_layer_weights.T\n",
    "hidden_layer_pre_activation_gradients = hidden_layer_activation_gradients * (hidden_layer_linear_output > 0)  # ReLU mask\n",
    "\n",
    "# 4) How much each hidden layer weight and bias contributed to the error\n",
    "hidden_layer_weight_gradients = input_feature_matrix.T @ hidden_layer_pre_activation_gradients\n",
    "hidden_layer_bias_gradients = np.sum(hidden_layer_pre_activation_gradients, axis=0, keepdims=True)\n",
    "\n",
    "# 5) Apply the changes to weights and biases (gradient descent)\n",
    "output_layer_weights -= learning_rate * output_layer_weight_gradients\n",
    "output_layer_bias    -= learning_rate * output_layer_bias_gradients\n",
    "hidden_layer_weights -= learning_rate * hidden_layer_weight_gradients\n",
    "hidden_layer_bias    -= learning_rate * hidden_layer_bias_gradients\n",
    "\n",
    "print(f\"\\nUpdated hidden layer weights matrix: {hidden_layer_weights.shape}\")\n",
    "print(hidden_layer_weights)\n",
    "print(f\"\\nUpdated output layer weights matrix: {output_layer_weights.shape}\")\n",
    "print(output_layer_weights)\n",
    "\n",
    "print(f\"\\nUpdated hidden layer bias: {hidden_layer_bias}\")\n",
    "print(f\"\\nUpdated output layer bias: {output_layer_bias}\")\n",
    "\n",
    "# TODO add these to functions to use in training loop"
   ],
   "id": "2b487c09f9faefb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated hidden layer weights matrix: (3, 4)\n",
      "[[ 0.49925292 -0.18701334  0.61420713  1.51211561]\n",
      " [-0.23170096 -0.2688049   1.56087143  0.76145582]\n",
      " [-0.46692844  0.49283164 -0.49736141 -0.4767947 ]]\n",
      "\n",
      "Updated output layer weights matrix: (4, 1)\n",
      "[[ 0.23986088]\n",
      " [-1.90694383]\n",
      " [-1.70467319]\n",
      " [-0.53372942]]\n",
      "\n",
      "Updated hidden layer bias: [[-0.00231686 -0.02991252 -0.03121038 -0.01017394]]\n",
      "\n",
      "Updated output layer bias: [[0.01232659]]\n"
     ]
    }
   ],
   "execution_count": 290
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "\n",
    "Here we will train the neural net to get the trained weights"
   ],
   "id": "2899eef89e536475"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T21:55:01.440229Z",
     "start_time": "2025-08-09T21:55:01.438913Z"
    }
   },
   "cell_type": "code",
   "source": "# TODO build the training loop using forward pass functions and back prop functions",
   "id": "d6d7f21f16d58b30",
   "outputs": [],
   "execution_count": 291
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Evaluation\n",
    "\n",
    "Lets evaluate and make sure the loss is decreasing and the trained weights will produce accurate classifications"
   ],
   "id": "b2ec212ad0be70f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T21:55:01.466115Z",
     "start_time": "2025-08-09T21:55:01.462745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Verify loss decreased\n",
    "hidden_layer_linear_output = hidden_layer_output_transformation(input_feature_matrix, hidden_layer_weights, hidden_layer_bias)\n",
    "hidden_layer_activation_output = hidden_ReLU_activation(hidden_layer_linear_output)\n",
    "output_layer_linear_output = output_layer_transformation(hidden_layer_activation_output, output_layer_weights, output_layer_bias)\n",
    "\n",
    "print(\"BCE loss first forward pass:\", first_forward_pass_loss)\n",
    "print(\"BCE loss second forward pass:\", BCE_loss(output_layer_linear_output, target_labels))\n",
    "\n",
    "# TODO: make predictions; feed real input data with classification meanings to the NN (e.g., try a small test set)"
   ],
   "id": "651fa1644959e0c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE loss first forward pass: 1.672443319506854\n",
      "BCE loss second forward pass: 1.4576551623423253\n"
     ]
    }
   ],
   "execution_count": 292
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
