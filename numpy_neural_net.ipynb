{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Classification Neural Net - NUMPY\n",
    "\n",
    "In this notebook I will develop a simple classification neural network from scratch using pythons NUMPY, instead of relying on libaries like pytorch."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T13:33:29.503917Z",
     "start_time": "2025-08-11T13:33:29.501377Z"
    }
   },
   "cell_type": "code",
   "source": "import numpy as np",
   "id": "807f76eb6a7a6401",
   "outputs": [],
   "execution_count": 321
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First we will define the hyperparameters that will be used throughout the notebook.",
   "id": "1580e508617d8033"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T13:33:29.525492Z",
     "start_time": "2025-08-11T13:33:29.523327Z"
    }
   },
   "cell_type": "code",
   "source": "learning_rate = 0.05",
   "id": "972f031b596341a7",
   "outputs": [],
   "execution_count": 322
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next, lets create some sample data to work with.\n",
    "- features: is (n_samples, n_features)\n",
    "- labels: is (n_samples, 1)\n"
   ],
   "id": "f8964909ebe70b81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T13:33:29.561039Z",
     "start_time": "2025-08-11T13:33:29.555344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# features: [height (m), weight (kg), score (0-10)]\n",
    "input_feature_matrix = np.array([\n",
    "    [1.80, 80, 8],   # good\n",
    "    [1.65, 70, 6],   # good\n",
    "    [1.75, 95, 5],   # bad\n",
    "    [1.60, 60, 4],   # bad\n",
    "    [1.82, 77, 9],   # good\n",
    "    [1.55, 50, 3],   # bad\n",
    "    [1.78, 85, 7],   # good\n",
    "    [1.62, 65, 2],   # bad\n",
    "], dtype=float)\n",
    "\n",
    "# normalize features (z-score)\n",
    "X = input_feature_matrix\n",
    "X = (X - X.mean(axis=0, keepdims=True)) / (X.std(axis=0, keepdims=True) + 1e-8)\n",
    "input_feature_matrix = X\n",
    "\n",
    "# labels: 1 = Good, 0 = Bad\n",
    "target_labels = np.array([\n",
    "    [1],\n",
    "    [1],\n",
    "    [0],\n",
    "    [0],\n",
    "    [1],\n",
    "    [0],\n",
    "    [1],\n",
    "    [0]\n",
    "], dtype=float)\n",
    "\n",
    "# random initial weights and biases\n",
    "np.random.seed(42)\n",
    "hidden_layer_weights = np.random.randn(input_feature_matrix.shape[1], 4)\n",
    "output_layer_weights = np.random.randn(4, 1)\n",
    "hidden_layer_bias = np.zeros((1, hidden_layer_weights.shape[1]))\n",
    "output_layer_bias = np.zeros((1, output_layer_weights.shape[1]))\n",
    "\n",
    "num_samples = input_feature_matrix.shape[0]\n",
    "\n",
    "print(f\"feature matrix shape: {input_feature_matrix.shape}\")\n",
    "print(input_feature_matrix)\n",
    "print(f\"hidden weights matrix shape: {hidden_layer_weights.shape}\")\n",
    "print(hidden_layer_weights)\n",
    "print(f\"output weights matrix shape: {output_layer_weights.shape}\")\n",
    "print(output_layer_weights)\n",
    "print(f\"labels matrix shape: {target_labels.shape}\")\n",
    "print(target_labels)"
   ],
   "id": "7f046f99ac7d0fc6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature matrix shape: (8, 3)\n",
      "[[ 1.07448419  0.53602696  1.09108945]\n",
      " [-0.47898693 -0.20332057  0.21821789]\n",
      " [ 0.55666048  1.64504827 -0.21821789]\n",
      " [-0.99681063 -0.94266811 -0.65465367]\n",
      " [ 1.28161367  0.3142227   1.52752522]\n",
      " [-1.51463434 -1.68201564 -1.09108945]\n",
      " [ 0.86735471  0.90570073  0.65465367]\n",
      " [-0.78968115 -0.57299434 -1.52752522]]\n",
      "hidden weights matrix shape: (3, 4)\n",
      "[[ 0.49671415 -0.1382643   0.64768854  1.52302986]\n",
      " [-0.23415337 -0.23413696  1.57921282  0.76743473]\n",
      " [-0.46947439  0.54256004 -0.46341769 -0.46572975]]\n",
      "output weights matrix shape: (4, 1)\n",
      "[[ 0.24196227]\n",
      " [-1.91328024]\n",
      " [-1.72491783]\n",
      " [-0.56228753]]\n",
      "labels matrix shape: (8, 1)\n",
      "[[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "execution_count": 323
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Neural Network ***Forward Pass*** – 1 Hidden Layer\n",
    "\n",
    "We will create functions for each part of the forward pass:\n",
    "1. **Hidden layer linear transformation** – multiply the feature matrix by the weight matrix, add bias, and produce the pre-activation values for the hidden layer.\n",
    "2. **Hidden layer activation (ReLU)** – introduce non-linearity so the network can learn complex patterns.\n",
    "3. **Output layer linear transformation** – take the hidden layer activations, multiply by the output layer weights, add bias, and produce the output logits.\n",
    "4. **Output layer activation (Sigmoid)** – squash the logits into the range (0, 1) to get probabilities.\n",
    "5. **Loss (MSE)** – measure how far the predicted values are from the target labels.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Hidden layer linear transformation\n",
    "$$\n",
    "Z1 = \\text{features\\_matrix} \\cdot \\text{weights\\_matrix} + \\text{bias}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `features_matrix` = input data `(n_samples, n_features)`\n",
    "- `weights_matrix` = hidden layer weights `(n_features, n_hidden)`\n",
    "- `bias` = hidden layer bias `(1, n_hidden)`\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Hidden layer activation (ReLU)\n",
    "$$\n",
    "A1 = \\max(0, Z1)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `A1` = hidden layer activation output `(n_samples, n_hidden)`\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Output layer linear transformation\n",
    "$$\n",
    "Z2 = A1 \\cdot W2 + b2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `W2` = output layer weights `(n_hidden, 1)`\n",
    "- `b2` = output layer bias `(1, 1)`\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Binary Cross-Entropy (BCE) Loss – from logits\n",
    "$$\n",
    "\\text{loss} = \\frac{1}{n_{\\text{samples}}} \\sum_{i=1}^{n_{\\text{samples}}}\n",
    "\\left[ \\max(z_i, 0) - z_i \\cdot \\text{labels}_i + \\log\\left( 1 + e^{-\\lvert z_i \\rvert} \\right) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `z` = logits `(n_samples, 1)` from the output layer transformation (before sigmoid)\n",
    "- `labels` = true labels `(n_samples, 1)`\n",
    "- `n_samples` = number of rows in `features_matrix`\n",
    "- This formulation is **numerically stable** and does **not** require applying the sigmoid in the forward pass.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "488cce66cffe9684"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will implement each formula above in order of how they're applied during the forward pass:",
   "id": "887a551efcb590a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T13:33:29.603230Z",
     "start_time": "2025-08-11T13:33:29.599619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def hidden_layer_output_transformation(input_feature_matrix, hidden_layer_weights, hidden_layer_bias):\n",
    "    return input_feature_matrix @ hidden_layer_weights + hidden_layer_bias\n",
    "\n",
    "def hidden_ReLU_activation(hidden_layer_linear_output):\n",
    "    return np.maximum(0, hidden_layer_linear_output)\n",
    "\n",
    "def output_layer_transformation(hidden_layer_activation_output, output_layer_weights, output_layer_bias):\n",
    "    return hidden_layer_activation_output @ output_layer_weights + output_layer_bias\n",
    "\n",
    "def BCE_loss(output_layer_linear_output, target_labels):\n",
    "    return np.mean(\n",
    "        np.maximum(output_layer_linear_output, 0)\n",
    "        - output_layer_linear_output * target_labels\n",
    "        + np.log1p(np.exp(-np.abs(output_layer_linear_output)))\n",
    "    )"
   ],
   "id": "9be632b40e61654",
   "outputs": [],
   "execution_count": 324
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Neural Network ***Backpropagation*** – 1 Hidden Layer (BCE from logits)\n",
    "\n",
    "We will compute gradients for each parameter using the chain rule, then update the weights and biases.\n",
    "1. **Gradient of loss w.r.t. output logits** – Figure out how much each output logit (before sigmoid) is pushing the loss up or down, so we know the direction to adjust them.\n",
    "2. **Gradient w.r.t. output layer weights & bias** – See how much each connection from the hidden layer to the output contributed to the error, so we can strengthen or weaken them.\n",
    "3. **Gradient w.r.t. hidden layer activations** – Work backwards to see how much the hidden neurons themselves are responsible for the error at the output.\n",
    "4. **Gradient w.r.t. hidden layer weights & bias** – Determine how much each connection from the inputs to the hidden neurons needs to be adjusted to fix the error.\n",
    "5. **Update weights & biases** – Apply the changes (scaled by the learning rate) so the network gets a bit better at predicting next time.\n",
    "---\n",
    "\n",
    "#### 1. Gradient of loss w.r.t. output layer logits\n",
    "$$\n",
    "dZ2 \\;=\\; \\frac{\\sigma(Z2) - \\text{labels}}{n_{\\text{samples}}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `Z2` = output logits `(n_samples, 1)`\n",
    "- `labels` = true labels `(n_samples, 1)`\n",
    "- `σ(·)` = sigmoid applied element-wise\n",
    "- `n_samples` = number of rows in `features_matrix`\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Output layer parameter gradients\n",
    "$$\n",
    "dW2 \\;=\\; A1^\\top \\cdot dZ2\n",
    "\\qquad\\qquad\n",
    "db2 \\;=\\; \\sum_{i=1}^{n_{\\text{samples}}} (dZ2)_i\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `A1` = hidden layer ReLU output `(n_samples, n_hidden)`\n",
    "- `W2` = output layer weights `(n_hidden, 1)`\n",
    "- `b2` = output layer bias `(1, 1)`\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Backprop into hidden activations\n",
    "$$\n",
    "dA1 \\;=\\; dZ2 \\cdot W2^\\top\n",
    "\\qquad\\qquad\n",
    "dZ1 \\;=\\; dA1 \\odot \\mathbf{1}(Z1 > 0)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `W2.T` = transpose of `W2` `(1, n_hidden)`\n",
    "- `⊙` denotes element-wise product\n",
    "- `Z1` = hidden pre-activations `(n_samples, n_hidden)`\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Hidden layer parameter gradients\n",
    "$$\n",
    "dW1 \\;=\\; X^\\top \\cdot dZ1\n",
    "\\qquad\\qquad\n",
    "db1 \\;=\\; \\sum_{i=1}^{n_{\\text{samples}}} (dZ1)_i\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `X` = input features `(n_samples, n_features)`\n",
    "- `W1` = hidden layer weights `(n_features, n_hidden)`\n",
    "- `b1` = hidden layer bias `(1, n_hidden)`\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Parameter updates (gradient descent)\n",
    "$$\n",
    "W1 \\leftarrow W1 - \\eta \\cdot dW1\n",
    "\\qquad\n",
    "b1 \\leftarrow b1 - \\eta \\cdot db1\n",
    "$$\n",
    "$$\n",
    "W2 \\leftarrow W2 - \\eta \\cdot dW2\n",
    "\\qquad\n",
    "b2 \\leftarrow b2 - \\eta \\cdot db2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `η` = learning rate `(scalar)`\n"
   ],
   "id": "be85d5fe489df2dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T13:33:29.650116Z",
     "start_time": "2025-08-11T13:33:29.645488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def stable_sigmoid(x):\n",
    "    pos = x >= 0\n",
    "    out = np.empty_like(x, dtype=float)\n",
    "    out[pos] = 1.0 / (1.0 + np.exp(-x[pos]))\n",
    "    e = np.exp(x[~pos])\n",
    "    out[~pos] = e / (1.0 + e)\n",
    "    return out\n",
    "\n",
    "def compute_loss_gradient_wrt_logits(output_layer_linear_output, target_labels):\n",
    "    num_samples = output_layer_linear_output.shape[0]\n",
    "    return (stable_sigmoid(output_layer_linear_output) - target_labels) / num_samples\n",
    "\n",
    "def output_layer_param_gradients(hidden_layer_ReLU_output, loss_gradient_wrt_logits):\n",
    "    output_layer_weight_gradients = hidden_layer_ReLU_output.T @ loss_gradient_wrt_logits\n",
    "    output_layer_bias_gradients   = np.sum(loss_gradient_wrt_logits, axis=0, keepdims=True)\n",
    "    return output_layer_weight_gradients, output_layer_bias_gradients\n",
    "\n",
    "def backprop_to_hidden(loss_gradient_wrt_logits, output_layer_weights, hidden_layer_linear_output):\n",
    "    hidden_layer_activation_gradients     = loss_gradient_wrt_logits @ output_layer_weights.T\n",
    "    hidden_layer_pre_activation_gradients = hidden_layer_activation_gradients * (hidden_layer_linear_output > 0)\n",
    "    return hidden_layer_pre_activation_gradients\n",
    "\n",
    "def hidden_layer_param_gradients(input_feature_matrix, hidden_layer_pre_activation_gradients):\n",
    "    hidden_layer_weight_gradients = input_feature_matrix.T @ hidden_layer_pre_activation_gradients\n",
    "    hidden_layer_bias_gradients   = np.sum(hidden_layer_pre_activation_gradients, axis=0, keepdims=True)\n",
    "    return hidden_layer_weight_gradients, hidden_layer_bias_gradients\n",
    "\n",
    "def apply_parameter_updates(hidden_layer_weights, hidden_layer_bias,\n",
    "                            output_layer_weights, output_layer_bias,\n",
    "                            hidden_layer_weight_gradients, hidden_layer_bias_gradients,\n",
    "                            output_layer_weight_gradients, output_layer_bias_gradients,\n",
    "                            learning_rate):\n",
    "    output_layer_weights -= learning_rate * output_layer_weight_gradients\n",
    "    output_layer_bias    -= learning_rate * output_layer_bias_gradients\n",
    "    hidden_layer_weights -= learning_rate * hidden_layer_weight_gradients\n",
    "    hidden_layer_bias    -= learning_rate * hidden_layer_bias_gradients\n",
    "    return hidden_layer_weights, hidden_layer_bias, output_layer_weights, output_layer_bias\n"
   ],
   "id": "2b487c09f9faefb4",
   "outputs": [],
   "execution_count": 325
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "\n",
    "Here we will train the neural net to get the trained weights"
   ],
   "id": "2899eef89e536475"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T13:33:29.681696Z",
     "start_time": "2025-08-11T13:33:29.669561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # 1) forward pass\n",
    "    hidden_layer_linear_output = hidden_layer_output_transformation(input_feature_matrix, hidden_layer_weights, hidden_layer_bias)\n",
    "    hidden_layer_ReLU_output = hidden_ReLU_activation(hidden_layer_linear_output)\n",
    "    output_layer_linear_output = output_layer_transformation(hidden_layer_ReLU_output, output_layer_weights, output_layer_bias)\n",
    "\n",
    "    # 2) compute loss\n",
    "    forward_pass_loss = BCE_loss(output_layer_linear_output, target_labels)\n",
    "    print(f\"Forward pass loss: {forward_pass_loss}\")\n",
    "\n",
    "    # 3) backward pass (compute gradients)\n",
    "    loss_gradient_wrt_logits = compute_loss_gradient_wrt_logits(output_layer_linear_output, target_labels)\n",
    "    output_layer_weight_gradients, output_layer_bias_gradients = output_layer_param_gradients(hidden_layer_ReLU_output, loss_gradient_wrt_logits)\n",
    "    hidden_layer_pre_activation_gradients = backprop_to_hidden(loss_gradient_wrt_logits, output_layer_weights, hidden_layer_linear_output)\n",
    "    hidden_layer_weight_gradients, hidden_layer_bias_gradients = hidden_layer_param_gradients(input_feature_matrix, hidden_layer_pre_activation_gradients)\n",
    "\n",
    "    # 4) parameter update\n",
    "    apply_parameter_updates(hidden_layer_weights, hidden_layer_bias,\n",
    "                            output_layer_weights, output_layer_bias,\n",
    "                            hidden_layer_weight_gradients, hidden_layer_bias_gradients,\n",
    "                            output_layer_weight_gradients, output_layer_bias_gradients,\n",
    "                            learning_rate)\n"
   ],
   "id": "d6d7f21f16d58b30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass loss: 1.672443319506854\n",
      "Forward pass loss: 1.4576551623423253\n",
      "Forward pass loss: 1.2916749313892337\n",
      "Forward pass loss: 1.1566915082738294\n",
      "Forward pass loss: 1.060687634863001\n",
      "Forward pass loss: 0.9771363823392609\n",
      "Forward pass loss: 0.9225672280312348\n",
      "Forward pass loss: 0.8863186738641593\n",
      "Forward pass loss: 0.8525682030873303\n",
      "Forward pass loss: 0.8211639804930416\n",
      "Forward pass loss: 0.7919575913373185\n",
      "Forward pass loss: 0.7648046745215458\n",
      "Forward pass loss: 0.7395656346300895\n",
      "Forward pass loss: 0.7161063356619484\n",
      "Forward pass loss: 0.694298706547937\n",
      "Forward pass loss: 0.6773253259494607\n",
      "Forward pass loss: 0.6642707813501343\n",
      "Forward pass loss: 0.6448822325261164\n",
      "Forward pass loss: 0.6268146012497637\n",
      "Forward pass loss: 0.6099223186249237\n",
      "Forward pass loss: 0.5940820523461696\n",
      "Forward pass loss: 0.579188621124995\n",
      "Forward pass loss: 0.5651517344764907\n",
      "Forward pass loss: 0.5519005278851353\n",
      "Forward pass loss: 0.5393705278894347\n",
      "Forward pass loss: 0.5274884259662774\n",
      "Forward pass loss: 0.5162027012618859\n",
      "Forward pass loss: 0.5054677162559161\n",
      "Forward pass loss: 0.4952428366462806\n",
      "Forward pass loss: 0.48549170637495365\n",
      "Forward pass loss: 0.4761816475269741\n",
      "Forward pass loss: 0.46728316116696994\n",
      "Forward pass loss: 0.45876951011554234\n",
      "Forward pass loss: 0.45061636852926107\n",
      "Forward pass loss: 0.44280152617739543\n",
      "Forward pass loss: 0.43530463769358496\n",
      "Forward pass loss: 0.4281070089654595\n",
      "Forward pass loss: 0.42119141432027446\n",
      "Forward pass loss: 0.4145419393549774\n",
      "Forward pass loss: 0.40814384521034097\n",
      "Forward pass loss: 0.40198345085166765\n",
      "Forward pass loss: 0.3960480305325028\n",
      "Forward pass loss: 0.390235714029859\n",
      "Forward pass loss: 0.38353043136475395\n",
      "Forward pass loss: 0.37713908805876123\n",
      "Forward pass loss: 0.371038121742953\n",
      "Forward pass loss: 0.365206355325813\n",
      "Forward pass loss: 0.35962469541286235\n",
      "Forward pass loss: 0.3542758739899114\n",
      "Forward pass loss: 0.3491442268929067\n",
      "Forward pass loss: 0.34421550351393954\n",
      "Forward pass loss: 0.33947670301629934\n",
      "Forward pass loss: 0.3349159330499879\n",
      "Forward pass loss: 0.3305222875782444\n",
      "Forward pass loss: 0.3262857409542438\n",
      "Forward pass loss: 0.3221970558355061\n",
      "Forward pass loss: 0.31824770290210325\n",
      "Forward pass loss: 0.314429790663332\n",
      "Forward pass loss: 0.31073600390509276\n",
      "Forward pass loss: 0.3071595495546744\n",
      "Forward pass loss: 0.30369410892785875\n",
      "Forward pass loss: 0.300333795481095\n",
      "Forward pass loss: 0.2970734202108941\n",
      "Forward pass loss: 0.29390936827105835\n",
      "Forward pass loss: 0.2908351596759523\n",
      "Forward pass loss: 0.2878462950769009\n",
      "Forward pass loss: 0.28493855594130085\n",
      "Forward pass loss: 0.28210798304322526\n",
      "Forward pass loss: 0.2793508570542884\n",
      "Forward pass loss: 0.2770074341206232\n",
      "Forward pass loss: 0.2745437726893625\n",
      "Forward pass loss: 0.2721139700745625\n",
      "Forward pass loss: 0.2698569265007939\n",
      "Forward pass loss: 0.26751852688031\n",
      "Forward pass loss: 0.2651893610777918\n",
      "Forward pass loss: 0.2630553274149666\n",
      "Forward pass loss: 0.26079196274238997\n",
      "Forward pass loss: 0.2586098970909106\n",
      "Forward pass loss: 0.25652144549474104\n",
      "Forward pass loss: 0.2543439455164123\n",
      "Forward pass loss: 0.25233496385172266\n",
      "Forward pass loss: 0.2502455324246582\n",
      "Forward pass loss: 0.24822902313440737\n",
      "Forward pass loss: 0.2462578928930533\n",
      "Forward pass loss: 0.24425049734550197\n",
      "Forward pass loss: 0.24237524535560923\n",
      "Forward pass loss: 0.24039198326398162\n",
      "Forward pass loss: 0.2385922900519769\n",
      "Forward pass loss: 0.23664669508471295\n",
      "Forward pass loss: 0.23490415832412898\n",
      "Forward pass loss: 0.23300840262428085\n",
      "Forward pass loss: 0.23130636876289937\n",
      "Forward pass loss: 0.22947137718377736\n",
      "Forward pass loss: 0.22779478900353453\n",
      "Forward pass loss: 0.22603034383916518\n",
      "Forward pass loss: 0.2243656022161487\n",
      "Forward pass loss: 0.22268043916887836\n",
      "Forward pass loss: 0.22101527752921404\n",
      "Forward pass loss: 0.21941717361607388\n",
      "Forward pass loss: 0.2177540982813407\n"
     ]
    }
   ],
   "execution_count": 326
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Evaluation\n",
    "\n",
    "Lets evaluate and make sure the loss is decreasing and the trained weights will produce accurate classifications"
   ],
   "id": "b2ec212ad0be70f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T13:33:29.707702Z",
     "start_time": "2025-08-11T13:33:29.702661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Verify loss decreased\n",
    "hidden_layer_linear_output = hidden_layer_output_transformation(input_feature_matrix, hidden_layer_weights, hidden_layer_bias)\n",
    "hidden_layer_ReLU_output = hidden_ReLU_activation(hidden_layer_linear_output)\n",
    "output_layer_linear_output = output_layer_transformation(hidden_layer_ReLU_output, output_layer_weights, output_layer_bias)\n",
    "\n",
    "print(\"BCE loss first forward pass:\", forward_pass_loss)\n",
    "\n",
    "# probs & preds\n",
    "probs = 1.0 / (1.0 + np.exp(-output_layer_linear_output))\n",
    "preds = (probs >= 0.5).astype(float)\n",
    "\n",
    "acc = (preds == target_labels).mean()\n",
    "print(\"final loss:\", BCE_loss(output_layer_linear_output, target_labels))\n",
    "print(\"final accuracy:\", acc)\n",
    "print(\"probs:\\n\", probs.ravel())\n",
    "print(\"preds:\\n\", preds.ravel())\n",
    "print(\"labels:\\n\", target_labels.ravel())\n"
   ],
   "id": "651fa1644959e0c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE loss first forward pass: 0.2177540982813407\n",
      "final loss: 0.2162249904805389\n",
      "final accuracy: 1.0\n",
      "probs:\n",
      " [0.76974516 0.5956664  0.1752454  0.13338143 0.76476179 0.02156242\n",
      " 0.7811936  0.07437165]\n",
      "preds:\n",
      " [1. 1. 0. 0. 1. 0. 1. 0.]\n",
      "labels:\n",
      " [1. 1. 0. 0. 1. 0. 1. 0.]\n"
     ]
    }
   ],
   "execution_count": 327
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As you can see, the model is learning and converging nicely and our predictions math the expected labels.",
   "id": "23a7c315a6eb8aaa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
