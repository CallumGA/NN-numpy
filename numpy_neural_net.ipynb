{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Classification Neural Net - NUMPY\n",
    "\n",
    "In this notebook I will develop a simple classification neural network from scratch using pythons NUMPY, instead of relying on libaries like pytorch."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T19:03:54.816985Z",
     "start_time": "2025-08-09T19:03:54.814803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from mpmath import sigmoid"
   ],
   "id": "807f76eb6a7a6401",
   "outputs": [],
   "execution_count": 144
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First we will define the constants that will be used throughout the notebook.",
   "id": "1580e508617d8033"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T19:03:54.832429Z",
     "start_time": "2025-08-09T19:03:54.830345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bias1 = 0\n",
    "bias2 = 0\n",
    "learning_rate = 0.1\n",
    "truth = 1 # the value we expect (the actual value that's labeled )"
   ],
   "id": "972f031b596341a7",
   "outputs": [],
   "execution_count": 145
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next, lets create some sample data to work with.\n",
    "- features: is (n_samples, n_features)\n",
    "- labels: is (n_samples, 1)\n"
   ],
   "id": "f8964909ebe70b81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T19:03:54.865725Z",
     "start_time": "2025-08-09T19:03:54.858836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 8\n",
    "n_features = 3\n",
    "features_matrix = np.random.rand(n_samples, n_features)\n",
    "labels = np.array([[0], [1], [0], [1], [0], [1], [0], [1]])\n",
    "weights1 = np.random.randn(n_features, 4)\n",
    "weights2 = np.random.randn(4, 1)\n",
    "bias1 = np.zeros((1, weights1.shape[1]))\n",
    "bias2 = np.zeros((1, weights2.shape[1]))\n",
    "\n",
    "print(f\"feature matrix shape: {features_matrix.shape}\")\n",
    "print(features_matrix)\n",
    "print(f\"weights matrix shape: {weights1.shape}\")\n",
    "print(weights1)\n",
    "print(f\"labels matrix shape: {labels.shape}\")\n",
    "print(labels)"
   ],
   "id": "7f046f99ac7d0fc6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature matrix shape: (8, 3)\n",
      "[[0.37454012 0.95071431 0.73199394]\n",
      " [0.59865848 0.15601864 0.15599452]\n",
      " [0.05808361 0.86617615 0.60111501]\n",
      " [0.70807258 0.02058449 0.96990985]\n",
      " [0.83244264 0.21233911 0.18182497]\n",
      " [0.18340451 0.30424224 0.52475643]\n",
      " [0.43194502 0.29122914 0.61185289]\n",
      " [0.13949386 0.29214465 0.36636184]]\n",
      "weights matrix shape: (3, 4)\n",
      "[[ 1.46564877 -0.2257763   0.0675282  -1.42474819]\n",
      " [-0.54438272  0.11092259 -1.15099358  0.37569802]\n",
      " [-0.60063869 -0.29169375 -0.60170661  1.85227818]]\n",
      "labels matrix shape: (8, 1)\n",
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "execution_count": 146
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Neural Network ***Forward Pass*** – 1 Hidden Layer\n",
    "\n",
    "We will create functions for each part of the forward pass:\n",
    "1. **Hidden layer linear transformation** – multiply the feature matrix by the weight matrix, add bias, and produce the pre-activation values for the hidden layer.\n",
    "2. **Hidden layer activation (ReLU)** – introduce non-linearity so the network can learn complex patterns.\n",
    "3. **Output layer linear transformation** – take the hidden layer activations, multiply by the output layer weights, add bias, and produce the output logits.\n",
    "4. **Output layer activation (Sigmoid)** – squash the logits into the range (0, 1) to get probabilities.\n",
    "5. **Loss (MSE)** – measure how far the predicted values are from the target labels.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Hidden layer linear transformation\n",
    "$$\n",
    "Z1 = \\text{features\\_matrix} \\cdot \\text{weights\\_matrix} + \\text{bias}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `features_matrix` = input data `(n_samples, n_features)`\n",
    "- `weights_matrix` = hidden layer weights `(n_features, n_hidden)`\n",
    "- `bias` = hidden layer bias `(1, n_hidden)`\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Hidden layer activation (ReLU)\n",
    "$$\n",
    "A1 = \\max(0, Z1)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `A1` = hidden layer activation output `(n_samples, n_hidden)`\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Output layer linear transformation\n",
    "$$\n",
    "Z2 = A1 \\cdot W2 + b2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `W2` = output layer weights `(n_hidden, 1)`\n",
    "- `b2` = output layer bias `(1, 1)`\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Binary Cross-Entropy (BCE) Loss – from logits\n",
    "$$\n",
    "\\text{loss} = \\frac{1}{n_{\\text{samples}}} \\sum_{i=1}^{n_{\\text{samples}}}\n",
    "\\left[ \\max(z_i, 0) - z_i \\cdot \\text{labels}_i + \\log\\left( 1 + e^{-\\lvert z_i \\rvert} \\right) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `z` = logits `(n_samples, 1)` from the output layer transformation (before sigmoid)\n",
    "- `labels` = true labels `(n_samples, 1)`\n",
    "- `n_samples` = number of rows in `features_matrix`\n",
    "- This formulation is **numerically stable** and does **not** require applying the sigmoid in the forward pass.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "488cce66cffe9684"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will implement each formula above in order of how they're applied during the forward pass:",
   "id": "887a551efcb590a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T19:03:54.895943Z",
     "start_time": "2025-08-09T19:03:54.890332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def hidden_layer_output_transformation(features_matrix, weights1, bias):\n",
    "    return features_matrix @ weights1 + bias\n",
    "\n",
    "def hidden_ReLU_activation(hidden_layer_output):\n",
    "    return np.maximum(0, hidden_layer_output)\n",
    "\n",
    "def output_layer_transformation(hidden_ReLU_activation, weights2, bias2):\n",
    "    return hidden_ReLU_activation @ weights2 + bias2\n",
    "\n",
    "def BCE_loss(logits, labels):\n",
    "    return np.mean(\n",
    "        np.maximum(logits, 0)\n",
    "        - logits * labels\n",
    "        + np.log1p(np.exp(-np.abs(logits)))\n",
    "    )\n",
    "print(\"Hidden layer transformation output: \")\n",
    "hidden_layer_output = hidden_layer_output_transformation(features_matrix, weights1, bias1)\n",
    "print(hidden_layer_output)\n",
    "\n",
    "print(\"Hidden layer ReLU activation: \")\n",
    "hidden_ReLU_activation_output = hidden_ReLU_activation(hidden_layer_output)\n",
    "print(hidden_ReLU_activation(hidden_layer_output))\n",
    "\n",
    "print(\"Output layer transformation output logits: \")\n",
    "output_layer_transformation_logits = output_layer_transformation(hidden_ReLU_activation_output, weights2, bias2)\n",
    "print(output_layer_transformation_logits)\n",
    "\n",
    "print(\"Final BCE loss:\")\n",
    "forward_pass_output = BCE_loss(output_layer_transformation_logits, labels)\n",
    "print(forward_pass_output)\n"
   ],
   "id": "9be632b40e61654",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer transformation output: \n",
      "[[-0.40827206 -0.19262465 -1.50941963  1.17941254]\n",
      " [ 0.69879287 -0.16335953 -0.23301305 -0.50537645]\n",
      " [-0.74745409 -0.09237689 -1.35473578  1.35609836]\n",
      " [ 0.44401448 -0.44049936 -0.55947892  0.79545129]\n",
      " [ 0.99526368 -0.21742982 -0.29759288 -0.76945534]\n",
      " [-0.21200664 -0.16072923 -0.65354531  0.82499286]\n",
      " [ 0.10703705 -0.24369272 -0.67419033  0.6273231 ]\n",
      " [-0.17464059 -0.10595443 -0.54727919  0.58961859]]\n",
      "Hidden layer ReLU activation: \n",
      "[[0.         0.         0.         1.17941254]\n",
      " [0.69879287 0.         0.         0.        ]\n",
      " [0.         0.         0.         1.35609836]\n",
      " [0.44401448 0.         0.         0.79545129]\n",
      " [0.99526368 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.82499286]\n",
      " [0.10703705 0.         0.         0.6273231 ]\n",
      " [0.         0.         0.         0.58961859]]\n",
      "Output layer transformation output logits: \n",
      "[[-1.4398783 ]\n",
      " [-0.00943176]\n",
      " [-1.65558408]\n",
      " [-0.97711462]\n",
      " [-0.0134333 ]\n",
      " [-1.00718729]\n",
      " [-0.76730812]\n",
      " [-0.71983211]]\n",
      "Final BCE loss:\n",
      "0.7355867441300599\n"
     ]
    }
   ],
   "execution_count": 147
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Neural Network ***Backpropagation*** – 1 Hidden Layer (BCE from logits)\n",
    "\n",
    "We will compute gradients for each parameter using the chain rule, then update the weights and biases.\n",
    "1. **Gradient of loss w.r.t. output logits** – Figure out how much each output logit (before sigmoid) is pushing the loss up or down, so we know the direction to adjust them.\n",
    "2. **Gradient w.r.t. output layer weights & bias** – See how much each connection from the hidden layer to the output contributed to the error, so we can strengthen or weaken them.\n",
    "3. **Gradient w.r.t. hidden layer activations** – Work backwards to see how much the hidden neurons themselves are responsible for the error at the output.\n",
    "4. **Gradient w.r.t. hidden layer weights & bias** – Determine how much each connection from the inputs to the hidden neurons needs to be adjusted to fix the error.\n",
    "5. **Update weights & biases** – Apply the changes (scaled by the learning rate) so the network gets a bit better at predicting next time.\n",
    "---\n",
    "\n",
    "#### 1. Gradient of loss w.r.t. output layer logits\n",
    "$$\n",
    "dZ2 \\;=\\; \\frac{\\sigma(Z2) - \\text{labels}}{n_{\\text{samples}}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `Z2` = output logits `(n_samples, 1)`\n",
    "- `labels` = true labels `(n_samples, 1)`\n",
    "- `σ(·)` = sigmoid applied element-wise\n",
    "- `n_samples` = number of rows in `features_matrix`\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Output layer parameter gradients\n",
    "$$\n",
    "dW2 \\;=\\; A1^\\top \\cdot dZ2\n",
    "\\qquad\\qquad\n",
    "db2 \\;=\\; \\sum_{i=1}^{n_{\\text{samples}}} (dZ2)_i\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `A1` = hidden layer ReLU output `(n_samples, n_hidden)`\n",
    "- `W2` = output layer weights `(n_hidden, 1)`\n",
    "- `b2` = output layer bias `(1, 1)`\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Backprop into hidden activations\n",
    "$$\n",
    "dA1 \\;=\\; dZ2 \\cdot W2^\\top\n",
    "\\qquad\\qquad\n",
    "dZ1 \\;=\\; dA1 \\odot \\mathbf{1}(Z1 > 0)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `W2.T` = transpose of `W2` `(1, n_hidden)`\n",
    "- `⊙` denotes element-wise product\n",
    "- `Z1` = hidden pre-activations `(n_samples, n_hidden)`\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Hidden layer parameter gradients\n",
    "$$\n",
    "dW1 \\;=\\; X^\\top \\cdot dZ1\n",
    "\\qquad\\qquad\n",
    "db1 \\;=\\; \\sum_{i=1}^{n_{\\text{samples}}} (dZ1)_i\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `X` = input features `(n_samples, n_features)`\n",
    "- `W1` = hidden layer weights `(n_features, n_hidden)`\n",
    "- `b1` = hidden layer bias `(1, n_hidden)`\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Parameter updates (gradient descent)\n",
    "$$\n",
    "W1 \\leftarrow W1 - \\eta \\cdot dW1\n",
    "\\qquad\n",
    "b1 \\leftarrow b1 - \\eta \\cdot db1\n",
    "$$\n",
    "$$\n",
    "W2 \\leftarrow W2 - \\eta \\cdot dW2\n",
    "\\qquad\n",
    "b2 \\leftarrow b2 - \\eta \\cdot db2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `η` = learning rate `(scalar)`\n"
   ],
   "id": "be85d5fe489df2dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T19:03:54.930673Z",
     "start_time": "2025-08-09T19:03:54.925222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1) How much each logit is pushing the loss up or down\n",
    "loss_gradient_wrt_logits = (\n",
    "    1.0 / (1.0 + np.exp(-output_layer_transformation_logits)) - labels\n",
    ") / n_samples\n",
    "print(\"Loss gradient w.r.t logits (output layer pre-activation):\\n\", loss_gradient_wrt_logits)\n",
    "\n",
    "# 2) How much each output layer weight and bias contributed to the error\n",
    "output_layer_weight_gradients = hidden_ReLU_activation_output.T @ loss_gradient_wrt_logits\n",
    "output_layer_bias_gradients = np.sum(loss_gradient_wrt_logits, axis=0, keepdims=True)\n",
    "print(\"\\nOutput layer weight gradients:\\n\", output_layer_weight_gradients)\n",
    "print(\"\\nOutput layer bias gradients:\\n\", output_layer_bias_gradients)\n",
    "\n",
    "# 3) How much each hidden activation was responsible for the output error ──> STUB\n",
    "\n",
    "# 4) How much each hidden layer weight and bias contributed to the error ──> STUB\n",
    "\n",
    "# 5) Apply the changes to weights and biases (gradient descent) ──> STUB\n",
    "\n"
   ],
   "id": "2b487c09f9faefb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss gradient w.r.t logits (output layer pre-activation):\n",
      " [[ 0.02394552]\n",
      " [-0.06279474]\n",
      " [ 0.02004446]\n",
      " [-0.09081691]\n",
      " [ 0.06208022]\n",
      " [-0.09155867]\n",
      " [ 0.03963271]\n",
      " [-0.08407126]]\n",
      "\n",
      "Output layer weight gradients:\n",
      " [[-0.01817619]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.11705923]]\n",
      "\n",
      "Output layer bias gradients:\n",
      " [[-0.18353867]]\n"
     ]
    }
   ],
   "execution_count": 148
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
